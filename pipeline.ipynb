{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82d9048f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install matplotlib\n",
    "# %pip install tqdm  \n",
    "from torch.utils.data import DataLoader\n",
    "from utils.dataset import VCTKDataset\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from src.model.cvae_tacotron_wrapper import CVAETacotron2, cvae_taco_loss\n",
    "import argparse\n",
    "from matplotlib import pyplot as plt\n",
    "import ipywidgets\n",
    "from tqdm.notebook import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8434970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected speakers: ['p323', 'p301', 'p240', 'p299', 'p225', 'p285', 'p252', 'p279', 'p287', 'p311']\n",
      "Speaker 0: p323: F, 19yo, SouthAfrican (Pretoria)\n",
      "Speaker 1: p301: F, 23yo, American (North Carolina)\n",
      "Speaker 2: p240: F, 21yo, English (Southern England)\n",
      "Speaker 3: p299: F, 25yo, American (California)\n",
      "Speaker 4: p225: F, 23yo, English (Southern England)\n",
      "Speaker 5: p285: M, 21yo, Scottish (Edinburgh)\n",
      "Speaker 6: p252: M, 22yo, Scottish (Edinburgh)\n",
      "Speaker 7: p279: M, 23yo, English (Leicester)\n",
      "Speaker 8: p287: M, 23yo, English (York)\n",
      "Speaker 9: p311: M, 21yo, American (Iowa)\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "dataset = VCTKDataset(\"./dataset/VCTK\",)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True, num_workers=0,\n",
    "                    collate_fn=VCTKDataset.collate_cvae)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c649a531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\jx/.cache\\torch\\hub\\NVIDIA_DeepLearningExamples_torchhub\n",
      "C:\\Users\\jx/.cache\\torch\\hub\\NVIDIA_DeepLearningExamples_torchhub\\PyTorch\\Classification\\ConvNets\\image_classification\\models\\common.py:13: UserWarning: pytorch_quantization module not found, quantization will not be available\n",
      "  warnings.warn(\n",
      "C:\\Users\\jx/.cache\\torch\\hub\\NVIDIA_DeepLearningExamples_torchhub\\PyTorch\\Classification\\ConvNets\\image_classification\\models\\efficientnet.py:17: UserWarning: pytorch_quantization module not found, quantization will not be available\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "423a90787d9e4fad8409da3d8d8646aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/5:   0%|          | 0/244 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch with text length tensor([48, 29, 36, 12, 91, 55, 34, 26, 32, 38, 61, 39, 27, 25, 47, 24]) and mel length tensor([422, 181, 251, 204, 449, 352, 229, 243, 284, 346, 366, 240, 280, 204,\n",
      "        370, 236])\n",
      "1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "`lengths` array must be sorted in decreasing order when `enforce_sorted` is True. You can pass `enforce_sorted=False` to pack_padded_sequence and/or pack_sequence to sidestep this requirement if you do not need ONNX exportability.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m     28\u001b[39m spk_emd = model.spk_emb[spk_id].to(device)\n\u001b[32m     29\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m mel_post, mel_out, gate_out, mu, logvar = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspk_emd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[32m2\u001b[39m)\n\u001b[32m     33\u001b[39m loss, logs = cvae_taco_loss(mel_post, mel, gate_out, gate, mu, logvar)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\ECE\\ECE1508\\PROJECT\\Personalized_Text-to-Speech\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\ECE\\ECE1508\\PROJECT\\Personalized_Text-to-Speech\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\ECE\\ECE1508\\PROJECT\\Personalized_Text-to-Speech\\src\\model\\cvae_tacotron_wrapper.py:95\u001b[39m, in \u001b[36mCVAETacotron2.forward\u001b[39m\u001b[34m(self, text_ids, text_lens, mel_gt, speaker_embed)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;66;03m# encoder: embed → conv → bLSTM (pretrained & frozen)\u001b[39;00m\n\u001b[32m     94\u001b[39m embedded = \u001b[38;5;28mself\u001b[39m.tts.embedding(text_ids).transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)  \u001b[38;5;66;03m# [B,txt_emb_size,L]\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m enc_out  = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtts\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_lens\u001b[49m\u001b[43m)\u001b[49m          \u001b[38;5;66;03m# [B,L,txt_emb_size]\u001b[39;00m\n\u001b[32m     96\u001b[39m enc_out  = enc_out + cond.unsqueeze(\u001b[32m1\u001b[39m)                   \u001b[38;5;66;03m# broadcast add\u001b[39;00m\n\u001b[32m     98\u001b[39m \u001b[38;5;66;03m# decoder teacher-forcing\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\ECE\\ECE1508\\PROJECT\\Personalized_Text-to-Speech\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\ECE\\ECE1508\\PROJECT\\Personalized_Text-to-Speech\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache\\torch\\hub\\NVIDIA_DeepLearningExamples_torchhub\\PyTorch/SpeechSynthesis/Tacotron2\\tacotron2\\model.py:219\u001b[39m, in \u001b[36mEncoder.forward\u001b[39m\u001b[34m(self, x, input_lengths)\u001b[39m\n\u001b[32m    217\u001b[39m \u001b[38;5;66;03m# pytorch tensor are not reversible, hence the conversion\u001b[39;00m\n\u001b[32m    218\u001b[39m input_lengths = input_lengths.cpu().numpy()\n\u001b[32m--> \u001b[39m\u001b[32m219\u001b[39m x = \u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mutils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpack_padded_sequence\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_lengths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_first\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[38;5;28mself\u001b[39m.lstm.flatten_parameters()\n\u001b[32m    223\u001b[39m outputs, _ = \u001b[38;5;28mself\u001b[39m.lstm(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\ECE\\ECE1508\\PROJECT\\Personalized_Text-to-Speech\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\rnn.py:335\u001b[39m, in \u001b[36mpack_padded_sequence\u001b[39m\u001b[34m(input, lengths, batch_first, enforce_sorted)\u001b[39m\n\u001b[32m    332\u001b[39m     batch_dim = \u001b[32m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batch_first \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m1\u001b[39m\n\u001b[32m    333\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[38;5;28minput\u001b[39m.index_select(batch_dim, sorted_indices)\n\u001b[32m--> \u001b[39m\u001b[32m335\u001b[39m data, batch_sizes = \u001b[43m_VF\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_pack_padded_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlengths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _packed_sequence_init(data, batch_sizes, sorted_indices, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[31mRuntimeError\u001b[39m: `lengths` array must be sorted in decreasing order when `enforce_sorted` is True. You can pass `enforce_sorted=False` to pack_padded_sequence and/or pack_sequence to sidestep this requirement if you do not need ONNX exportability."
     ]
    }
   ],
   "source": [
    "EPOCH = 5\n",
    "BATCH_SIZE = 32\n",
    "LR = 1e-3\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = CVAETacotron2(ckpt_path=\"./src/model/tacotron2_pretrained.pt\", z_dim=64, spk_dim_raw=256, spk_dim_proj=128)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "model.train()\n",
    "\n",
    "loss_tracker = []\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    loss_sum = 0\n",
    "    logsum = {'l1': 0, 'gate': 0, 'kl': 0}\n",
    "\n",
    "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{EPOCH}\", leave=False, position=0, dynamic_ncols=True)\n",
    "\n",
    "    for text, text_len, mel, mel_len, gate, spk_id in pbar:\n",
    "        print(f\"Processing batch with text length {text_len} and mel length {mel_len}\")\n",
    "        text, text_len = text.to(device), text_len.to(device)\n",
    "        mel, gate = mel.to(device), gate.to(device)\n",
    "        spk_emd = model.spk_emb[spk_id].to(device)\n",
    "        print(1)\n",
    "\n",
    "        mel_post, mel_out, gate_out, mu, logvar = model(text, text_len, mel, spk_emd)\n",
    "        print(2)\n",
    "        loss, logs = cvae_taco_loss(mel_post, mel, gate_out, gate, mu, logvar)\n",
    "        print(3)\n",
    "        \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_sum += loss.item()\n",
    "        logsum['l1'] += logs['l1']\n",
    "        logsum['gate'] += logs['gate']\n",
    "        logsum['kl'] += logs['kl']\n",
    "\n",
    "        pbar.set_postfix(loss=loss.item(), l1=logs['l1'], gate=logs['gate'], kl=logs['kl'])\n",
    "        break\n",
    "\n",
    "    loss_avg = loss_sum / len(dataloader)\n",
    "    loss_tracker.append(loss_avg)\n",
    "    print(f\"[Epoch {epoch+1}/{EPOCH}] Loss: {loss_avg:.4f} | l1: {logsum['l1'] / len(dataloader):.4f}, gate: {logsum['gate'] / len(dataloader):.4f}, kl: {logsum['kl'] / len(dataloader):.4f}\")\n",
    "\n",
    "plt.plot(loss_tracker)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "model_save_path = Path(\"./src/model/cvae_tacotron2_trial1.pth\")\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9277f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "text, text_len, mel, mel_len, gate, spk_id = next(iter(dataloader))\n",
    "text, text_len = text.to(device), text_len.to(device)\n",
    "mel, gate = mel.to(device), gate.to(device)\n",
    "spk_emd = model.spk_emb[spk_id].to(device)\n",
    "mel_post, mel_out, gate_out, mu, logvar = model(text, text_len, mel, spk_emd)\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(mel_post[0].cpu().detach().numpy(), aspect='auto', origin='lower')\n",
    "plt.title('Mel Post')\n",
    "plt.subplot(1, 2, 2)\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
